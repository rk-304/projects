{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard libraries for data analysis:\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, skew\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "# sklearn modules for data preprocessing:\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#sklearn modules for Model Selection:\n",
    "from sklearn import svm, tree, linear_model, neighbors\n",
    "from sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#sklearn modules for Model Evaluation & Improvement:\n",
    "    \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.metrics import auc, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import make_scorer, recall_score, log_loss\n",
    "from sklearn.metrics import average_precision_score\n",
    "#Standard libraries for data visualization:\n",
    "import seaborn as sn\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "color = sn.color_palette()\n",
    "import matplotlib.ticker as mtick\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "#Miscellaneous Utilitiy Libraries:\n",
    "    \n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import timeit\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from dateutil.parser import parse\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncals</th>\n",
       "      <th>nlogins</th>\n",
       "      <th>prop_withdrawn</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>wealth</th>\n",
       "      <th>sex</th>\n",
       "      <th>nprod</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0.208455</td>\n",
       "      <td>67.562479</td>\n",
       "      <td>3136.940631</td>\n",
       "      <td>100655.43210</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "      <td>0.064725</td>\n",
       "      <td>66.005370</td>\n",
       "      <td>1775.551353</td>\n",
       "      <td>111804.95730</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>0.508327</td>\n",
       "      <td>78.108875</td>\n",
       "      <td>7625.352377</td>\n",
       "      <td>163194.06550</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>0.317960</td>\n",
       "      <td>52.563260</td>\n",
       "      <td>2270.282638</td>\n",
       "      <td>105852.54840</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>298</td>\n",
       "      <td>0.295639</td>\n",
       "      <td>32.023677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34517.82859</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0.672235</td>\n",
       "      <td>34.748834</td>\n",
       "      <td>12266.026870</td>\n",
       "      <td>107480.15090</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>0.567027</td>\n",
       "      <td>35.720864</td>\n",
       "      <td>4845.930340</td>\n",
       "      <td>68255.80653</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>1</td>\n",
       "      <td>238</td>\n",
       "      <td>0.420206</td>\n",
       "      <td>52.013070</td>\n",
       "      <td>7046.942075</td>\n",
       "      <td>220682.45870</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>0.658794</td>\n",
       "      <td>36.488840</td>\n",
       "      <td>8660.932303</td>\n",
       "      <td>99476.20312</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0.557602</td>\n",
       "      <td>48.174720</td>\n",
       "      <td>5362.272554</td>\n",
       "      <td>147259.41970</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ncals  nlogins  prop_withdrawn        age        income        wealth  \\\n",
       "0        0       67        0.208455  67.562479   3136.940631  100655.43210   \n",
       "1        2      288        0.064725  66.005370   1775.551353  111804.95730   \n",
       "2        2       45        0.508327  78.108875   7625.352377  163194.06550   \n",
       "3        0      113        0.317960  52.563260   2270.282638  105852.54840   \n",
       "4        8      298        0.295639  32.023677      0.000000   34517.82859   \n",
       "..     ...      ...             ...        ...           ...           ...   \n",
       "595      2      174        0.672235  34.748834  12266.026870  107480.15090   \n",
       "596      0      227        0.567027  35.720864   4845.930340   68255.80653   \n",
       "597      1      238        0.420206  52.013070   7046.942075  220682.45870   \n",
       "598      1      167        0.658794  36.488840   8660.932303   99476.20312   \n",
       "599      0      127        0.557602  48.174720   5362.272554  147259.41970   \n",
       "\n",
       "     sex  nprod  churn  \n",
       "0      0      1      0  \n",
       "1      0      2      0  \n",
       "2      1      2      1  \n",
       "3      1      3      1  \n",
       "4      1      2      1  \n",
       "..   ...    ...    ...  \n",
       "595    1      4      0  \n",
       "596    1      2      1  \n",
       "597    1      4      0  \n",
       "598    1      4      1  \n",
       "599    1      4      1  \n",
       "\n",
       "[600 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('pred_challenge_train_data.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ncals', 'nlogins', 'prop_withdrawn', 'age', 'income', 'wealth', 'sex',\n",
       "       'nprod', 'churn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncals</th>\n",
       "      <th>nlogins</th>\n",
       "      <th>prop_withdrawn</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>wealth</th>\n",
       "      <th>sex</th>\n",
       "      <th>nprod</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>2.121667</td>\n",
       "      <td>145.816667</td>\n",
       "      <td>0.367417</td>\n",
       "      <td>52.686162</td>\n",
       "      <td>6196.204612</td>\n",
       "      <td>130801.219699</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>3.038333</td>\n",
       "      <td>0.488333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>2.530653</td>\n",
       "      <td>91.421348</td>\n",
       "      <td>0.197460</td>\n",
       "      <td>19.556128</td>\n",
       "      <td>2925.589774</td>\n",
       "      <td>51373.520087</td>\n",
       "      <td>0.500392</td>\n",
       "      <td>1.408369</td>\n",
       "      <td>0.500281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>20.001609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-11862.931160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.201889</td>\n",
       "      <td>36.440406</td>\n",
       "      <td>4161.108476</td>\n",
       "      <td>96991.105828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>145.500000</td>\n",
       "      <td>0.369472</td>\n",
       "      <td>49.541532</td>\n",
       "      <td>6264.503838</td>\n",
       "      <td>131147.224600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>0.546711</td>\n",
       "      <td>69.807062</td>\n",
       "      <td>8288.955803</td>\n",
       "      <td>162658.946100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1198.000000</td>\n",
       "      <td>0.870069</td>\n",
       "      <td>89.776055</td>\n",
       "      <td>14857.724500</td>\n",
       "      <td>280889.178000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ncals      nlogins  prop_withdrawn         age        income  \\\n",
       "count  600.000000   600.000000      600.000000  600.000000    600.000000   \n",
       "mean     2.121667   145.816667        0.367417   52.686162   6196.204612   \n",
       "std      2.530653    91.421348        0.197460   19.556128   2925.589774   \n",
       "min      0.000000     4.000000        0.001336   20.001609      0.000000   \n",
       "25%      0.750000    88.000000        0.201889   36.440406   4161.108476   \n",
       "50%      1.000000   145.500000        0.369472   49.541532   6264.503838   \n",
       "75%      2.000000   189.000000        0.546711   69.807062   8288.955803   \n",
       "max     14.000000  1198.000000        0.870069   89.776055  14857.724500   \n",
       "\n",
       "              wealth         sex       nprod       churn  \n",
       "count     600.000000  600.000000  600.000000  600.000000  \n",
       "mean   130801.219699    0.505000    3.038333    0.488333  \n",
       "std     51373.520087    0.500392    1.408369    0.500281  \n",
       "min    -11862.931160    0.000000    1.000000    0.000000  \n",
       "25%     96991.105828    0.000000    2.000000    0.000000  \n",
       "50%    131147.224600    1.000000    3.000000    0.000000  \n",
       "75%    162658.946100    1.000000    4.000000    1.000000  \n",
       "max    280889.178000    1.000000    5.000000    1.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ncals               int64\n",
       "nlogins             int64\n",
       "prop_withdrawn    float64\n",
       "age               float64\n",
       "income            float64\n",
       "wealth            float64\n",
       "sex                 int64\n",
       "nprod               int64\n",
       "churn               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'): Index(['ncals', 'nlogins', 'sex', 'nprod', 'churn'], dtype='object'),\n",
       " dtype('float64'): Index(['prop_withdrawn', 'age', 'income', 'wealth'], dtype='object')}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns.to_series().groupby(dataset.dtypes).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600 entries, 0 to 599\n",
      "Data columns (total 9 columns):\n",
      "ncals             600 non-null int64\n",
      "nlogins           600 non-null int64\n",
      "prop_withdrawn    600 non-null float64\n",
      "age               600 non-null float64\n",
      "income            600 non-null float64\n",
      "wealth            600 non-null float64\n",
      "sex               600 non-null int64\n",
      "nprod             600 non-null int64\n",
      "churn             600 non-null int64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 42.3 KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ncals             False\n",
       "nlogins           False\n",
       "prop_withdrawn    False\n",
       "age               False\n",
       "income            False\n",
       "wealth            False\n",
       "sex               False\n",
       "nprod             False\n",
       "churn             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset[['ncals', 'nlogins', 'sex', 'nprod', 'churn', 'prop_withdrawn', 'age', 'income', 'wealth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x864 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(15, 12))\n",
    "plt.suptitle('Histograms of Numerical Columns\\n',horizontalalignment=\"center\",fontstyle = \"normal\", fontsize = 24, fontfamily = \"sans-serif\")\n",
    "for i in range(dataset2.shape[1]):\n",
    "    plt.subplot(6, 3, i + 1)\n",
    "    f = plt.gca()\n",
    "    f.set_title(dataset2.columns.values[i])\n",
    "    \n",
    "vals = np.size(dataset2.iloc[:, i].unique())\n",
    "if vals >= 100:\n",
    "    vals = 100\n",
    "    \n",
    "plt.hist(dataset2.iloc[:, i], bins=vals, color = '#ec838a')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ncals</td>\n",
       "      <td>1.818774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nlogins</td>\n",
       "      <td>3.610749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sex</td>\n",
       "      <td>1.981648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>nprod</td>\n",
       "      <td>5.138905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>churn</td>\n",
       "      <td>2.233350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>prop_withdrawn</td>\n",
       "      <td>5.086571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>age</td>\n",
       "      <td>6.818432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>income</td>\n",
       "      <td>6.049172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>wealth</td>\n",
       "      <td>7.202229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        variables       VIF\n",
       "0           ncals  1.818774\n",
       "1         nlogins  3.610749\n",
       "2             sex  1.981648\n",
       "3           nprod  5.138905\n",
       "4           churn  2.233350\n",
       "5  prop_withdrawn  5.086571\n",
       "6             age  6.818432\n",
       "7          income  6.049172\n",
       "8          wealth  7.202229"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_vif(X):\n",
    "# Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) \n",
    "    for i in range(X.shape[1])]\n",
    "    return(vif)\n",
    "\n",
    "calc_vif(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = dataset[\"churn\"]\n",
    "dataset = dataset.drop(columns=\"churn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = pd.read_csv('pred_challenge_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ncals</th>\n",
       "      <th>nlogins</th>\n",
       "      <th>prop_withdrawn</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>wealth</th>\n",
       "      <th>sex</th>\n",
       "      <th>nprod</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>405</td>\n",
       "      <td>0.188828</td>\n",
       "      <td>26.636221</td>\n",
       "      <td>5664.680215</td>\n",
       "      <td>185549.86440</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0.207633</td>\n",
       "      <td>46.323303</td>\n",
       "      <td>5204.124936</td>\n",
       "      <td>62966.57907</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>0.025344</td>\n",
       "      <td>44.996767</td>\n",
       "      <td>7629.158148</td>\n",
       "      <td>201423.32940</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>0.634990</td>\n",
       "      <td>50.251881</td>\n",
       "      <td>9434.635616</td>\n",
       "      <td>118794.09680</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>184</td>\n",
       "      <td>0.497226</td>\n",
       "      <td>50.209373</td>\n",
       "      <td>6169.703119</td>\n",
       "      <td>165793.08760</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>0.527424</td>\n",
       "      <td>49.133755</td>\n",
       "      <td>3962.008725</td>\n",
       "      <td>92985.06043</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>1</td>\n",
       "      <td>176</td>\n",
       "      <td>0.297042</td>\n",
       "      <td>52.487664</td>\n",
       "      <td>7140.152272</td>\n",
       "      <td>158667.28460</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>11</td>\n",
       "      <td>121</td>\n",
       "      <td>0.318611</td>\n",
       "      <td>50.426094</td>\n",
       "      <td>6711.167659</td>\n",
       "      <td>177404.62490</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>0.060509</td>\n",
       "      <td>44.164528</td>\n",
       "      <td>4203.112212</td>\n",
       "      <td>93921.48389</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>0.559367</td>\n",
       "      <td>27.448991</td>\n",
       "      <td>9177.715285</td>\n",
       "      <td>123265.65150</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ncals  nlogins  prop_withdrawn        age       income        wealth  \\\n",
       "0        1      405        0.188828  26.636221  5664.680215  185549.86440   \n",
       "1        0      138        0.207633  46.323303  5204.124936   62966.57907   \n",
       "2        1      142        0.025344  44.996767  7629.158148  201423.32940   \n",
       "3        1      145        0.634990  50.251881  9434.635616  118794.09680   \n",
       "4        5      184        0.497226  50.209373  6169.703119  165793.08760   \n",
       "..     ...      ...             ...        ...          ...           ...   \n",
       "395      0      246        0.527424  49.133755  3962.008725   92985.06043   \n",
       "396      1      176        0.297042  52.487664  7140.152272  158667.28460   \n",
       "397     11      121        0.318611  50.426094  6711.167659  177404.62490   \n",
       "398      1      165        0.060509  44.164528  4203.112212   93921.48389   \n",
       "399      0      189        0.559367  27.448991  9177.715285  123265.65150   \n",
       "\n",
       "     sex  nprod  churn  \n",
       "0      1      1    NaN  \n",
       "1      1      5    NaN  \n",
       "2      1      3    NaN  \n",
       "3      1      5    NaN  \n",
       "4      0      3    NaN  \n",
       "..   ...    ...    ...  \n",
       "395    1      2    NaN  \n",
       "396    1      5    NaN  \n",
       "397    1      3    NaN  \n",
       "398    0      2    NaN  \n",
       "399    1      3    NaN  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = dataset3[\"churn\"]\n",
    "dataset3 = dataset3.drop(columns=\"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "595    0\n",
       "596    1\n",
       "597    0\n",
       "598    1\n",
       "599    1\n",
       "Name: churn, Length: 600, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (480, 8)\n",
      "Number transactions y_train dataset:  (480,)\n",
      "Number transactions X_test dataset:  (120, 8)\n",
      "Number transactions y_test dataset:  (120,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, response, stratify=response, test_size = 0.2, random_state = 0) #use 0.9 if data is huge.random_state = 0)\n",
    "#to resolve any class imbalance - use stratify parameter.\n",
    "print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-65627e782367>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(dataset, response, stratify=response, test_size = 0.2, random_state = 0)\n",
    "y_train, y_test = train_test_split(dataset3, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\n",
    "X_train2.columns = X_train.columns.values\n",
    "X_train2.index = X_train.index.values\n",
    "X_train = X_train2\n",
    "X_test2 = pd.DataFrame(sc_X.transform(X_test))\n",
    "X_test2.columns = X_test.columns.values\n",
    "X_test2.index = X_test.index.values\n",
    "X_test = X_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state = 0,\n",
    "                                                         class_weight='balanced')))\n",
    "models.append(('SVC', SVC(kernel = 'linear', random_state = 0)))\n",
    "models.append(('Kernel SVM', SVC(kernel = 'rbf', random_state = 0)))\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)))\n",
    "models.append(('Gaussian NB', GaussianNB()))\n",
    "models.append(('Decision Tree Classifier',\n",
    "               DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))\n",
    "models.append(('Random Forest', RandomForestClassifier(\n",
    "    n_estimators=100, criterion = 'entropy', random_state = 0)))\n",
    "#Evaluating Model Results:\n",
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "# set table to table to populate with performance results\n",
    "col = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', \n",
    "       'Accuracy Mean', 'Accuracy STD']\n",
    "model_results = pd.DataFrame(columns=col)\n",
    "i = 0\n",
    "# Evaluate each model using k-fold cross-validation:\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=10, random_state=0)\n",
    "# accuracy scoring:\n",
    "    cv_acc_results = model_selection.cross_val_score(  \n",
    "    model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    # roc_auc scoring:\n",
    "    cv_auc_results = model_selection.cross_val_score(  \n",
    "    model, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "    acc_results.append(cv_acc_results)\n",
    "    auc_results.append(cv_auc_results)\n",
    "    names.append(name)\n",
    "    model_results.loc[i] = [name,\n",
    "                             round(cv_auc_results.mean()*100, 2),\n",
    "                             round(cv_auc_results.std()*100, 2),\n",
    "                             round(cv_acc_results.mean()*100, 2),\n",
    "                             round(cv_acc_results.std()*100, 2)\n",
    "                             ]\n",
    "    i += 1\n",
    "    \n",
    "model_results.sort_values(by=['ROC AUC Mean'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(acc_results)\n",
    "ax.set_xticklabels(names)\n",
    "#plt.ylabel\n",
    "(horizontalalignment = \"center\", fontstyle = \"normal\", fontsize = \"large\", fontfamily = \"sans-serif\")\n",
    "#plt.xlabel\n",
    "('\\n Baseline Classification Algorithms\\n',horizontalalignment=\"center\",fontstyle = \"normal\", \n",
    "fontsize = \"large\", fontfamily = \"sans-serif\")\n",
    "plt.title('Accuracy Score Comparison \\n',horizontalalignment=\"center\", fontstyle = \"normal\", fontsize = \"22\", fontfamily = \"sans-serif\")\n",
    "#plt.legend\n",
    "(loc='top right', fontsize = \"medium\")\n",
    "plt.xticks(rotation=0, horizontalalignment=\"center\")\n",
    "plt.yticks(rotation=0, horizontalalignment=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(acc_results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.xticks(rotation=0, horizontalalignment=\"center\")\n",
    "plt.yticks(rotation=0, horizontalalignment=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "#Evaluate results\n",
    "acc = accuracy_score(y_test, y_pred )\n",
    "prec = precision_score(y_test, y_pred )\n",
    "rec = recall_score(y_test, y_pred )\n",
    "f1 = f1_score(y_test, y_pred )\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2.0)\n",
    "results = pd.DataFrame([['Logistic Regression', \n",
    "acc, prec, rec, f1, f2]], columns = ['Model', \n",
    "'Accuracy', 'Precision', 'Recall', 'F1 Score', \n",
    "'F2 Score'])\n",
    "results = results.sort_values([\"Precision\", \n",
    "\"Recall\", \"F2 Score\"], ascending = False)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting SVM (SVC class) to the Training set\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results y_pred = classifier.predict(X_test)\n",
    "#Evaluate results\n",
    "acc = accuracy_score(y_test, y_pred )\n",
    "prec = precision_score(y_test, y_pred )\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred )\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2.0)\n",
    "model_results = pd.DataFrame(\n",
    "[['SVM (Linear)', acc, prec, rec, f1, f2]],\n",
    "columns = ['Model', 'Accuracy', 'Precision', \n",
    "'Recall', 'F1 Score', 'F2 Score'])\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "results = results.sort_values([\"Precision\", \n",
    "\"Recall\", \"F2 Score\"], ascending = False)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting KNN to the Training set:\n",
    "classifier = KNeighborsClassifier(\n",
    "n_neighbors = 22, \n",
    "metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results \n",
    "y_pred  = classifier.predict(X_test)\n",
    "#Evaluate results\n",
    "acc = accuracy_score(y_test, y_pred )\n",
    "prec = precision_score(y_test, y_pred )\n",
    "rec = recall_score(y_test, y_pred )\n",
    "f1 = f1_score(y_test, y_pred )\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2.0)\n",
    "model_results = pd.DataFrame([['K-Nearest Neighbours', \n",
    "acc, prec, rec, f1, f2]], columns = ['Model',\n",
    " 'Accuracy', 'Precision', 'Recall',\n",
    " 'F1 Score', 'F2 Score'])\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "results = results.sort_values([\"Precision\", \n",
    "\"Recall\", \"F2 Score\"], ascending = False)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Kernel SVM to the Training set:\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "#Evaluate results\n",
    "acc = accuracy_score(y_test, y_pred )\n",
    "prec = precision_score(y_test, y_pred )\n",
    "rec = recall_score(y_test, y_pred )\n",
    "f1 = f1_score(y_test, y_pred )\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2.0)\n",
    "model_results = pd.DataFrame([[\n",
    "'Kernel SVM', acc, prec, rec, f1, f2]],\n",
    "columns = ['Model', 'Accuracy', 'Precision', \n",
    "'Recall', 'F1 Score', 'F2 Score'])\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "results = results.sort_values([\"Precision\", \n",
    "\"Recall\", \"F2 Score\"], ascending = False)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Naive Byes to the Training set:\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "#Evaluate results\n",
    "acc = accuracy_score(y_test, y_pred )\n",
    "prec = precision_score(y_test, y_pred )\n",
    "rec = recall_score(y_test, y_pred )\n",
    "f1 = f1_score(y_test, y_pred )\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2.0)\n",
    "model_results = pd.DataFrame([[\n",
    "'Naive Byes', acc, prec, rec, f1, f2]],\n",
    "columns = ['Model', 'Accuracy', 'Precision',\n",
    "'Recall', 'F1 Score', 'F2 Score'])\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "results = results.sort_values([\"Precision\", \n",
    "\"Recall\", \"F2 Score\"], ascending = False)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Decision Tree to the Training set:\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "#Evaluate results\n",
    "acc = accuracy_score(y_test, y_pred )\n",
    "prec = precision_score(y_test, y_pred )\n",
    "rec = recall_score(y_test, y_pred )\n",
    "f1 = f1_score(y_test, y_pred )\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2.0)\n",
    "model_results = pd.DataFrame([[\n",
    "'Decision Tree', acc, prec, rec, f1, f2]],\n",
    " columns = ['Model', 'Accuracy', 'Precision', \n",
    "'Recall', 'F1 Score', 'F2 Score'])\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "results = results.sort_values([\"Precision\", \n",
    "\"Recall\", \"F2 Score\"], ascending = False)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Random Forest to the Training set:\n",
    "    \n",
    "classifier = RandomForestClassifier(n_estimators = 72, \n",
    "criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predicting the Test set results \n",
    "y_pred = classifier.predict(X_test)\n",
    "#Evaluate results\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "acc = accuracy_score(y_test, y_pred )\n",
    "prec = precision_score(y_test, y_pred )\n",
    "rec = recall_score(y_test, y_pred )\n",
    "f1 = f1_score(y_test, y_pred )\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2.0)\n",
    "model_results = pd.DataFrame([['Random Forest', \n",
    "acc, prec, rec, f1, f2]],\n",
    "columns = ['Model', 'Accuracy', 'Precision', \n",
    "'Recall', 'F1 Score', 'F2 Score'])\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "results = results.sort_values([\"Precision\", \n",
    "\"Recall\", \"F2 Score\"], ascending = False)\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state = 0,\n",
    "penalty = 'l2')\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predict the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "#Evaluate Model Results on Test Set:\n",
    "acc = accuracy_score(y_test, y_pred )\n",
    "prec = precision_score(y_test, y_pred )\n",
    "rec = recall_score(y_test, y_pred )\n",
    "f1 = f1_score(y_test, y_pred )\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2.0)\n",
    "results = pd.DataFrame([['Logistic Regression',\n",
    "acc, prec, rec, f1, f2]],columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'F2 Score'])\n",
    "print (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = cross_val_score(estimator = classifier,\n",
    " X = X_train, y = y_train, cv = 10)\n",
    "print\n",
    "(\"Logistic Regression Classifier Accuracy: %0.2f (+/- %0.2f)\"  % (accuracies.mean(), accuracies.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred) \n",
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "plt.figure(figsize = (28,20))\n",
    "fig, ax = plt.subplots()\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, fmt='g'#,cmap=\"YlGnBu\" \n",
    "           )\n",
    "class_names=[0,1]\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix\\n', y=1.1)\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.ylabel('Actual label\\n')\n",
    "plt.xlabel('Predicted label\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train) \n",
    "probs = classifier.predict_proba(X_test) \n",
    "probs = probs[:, 1] \n",
    "classifier_roc_auc = accuracy_score(y_test, y_pred )\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, classifier.predict_proba(X_test)[:,1])\n",
    "plt.figure(figsize=(14, 6))\n",
    "# Plot Logistic Regression ROC\n",
    "plt.plot(rf_fpr, rf_tpr, \n",
    "label='Logistic Regression (area = %0.2f)' % classifier_roc_auc)\n",
    "# Plot Base Rate ROC\n",
    "plt.plot([0,1], [0,1],label='Base Rate' 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel('True Positive Rate \\n',horizontalalignment=\"center\",\n",
    "fontstyle = \"normal\", fontsize = \"medium\", \n",
    "fontfamily = \"sans-serif\")\n",
    "plt.xlabel('\\nFalse Positive Rate \\n',horizontalalignment=\"center\",\n",
    "fontstyle = \"normal\", fontsize = \"medium\", \n",
    "fontfamily = \"sans-serif\")\n",
    "plt.title('ROC Graph \\n',horizontalalignment=\"center\", \n",
    "fontstyle = \"normal\", fontsize = \"22\", \n",
    "fontfamily = \"sans-serif\")\n",
    "plt.legend(loc=\"lower right\", fontsize = \"medium\")\n",
    "plt.xticks(rotation=0, horizontalalignment=\"center\")\n",
    "plt.yticks(rotation=0, horizontalalignment=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Coefficients\n",
    "feature_importances = pd.concat\n",
    "([columns = columns,\n",
    "pd.DataFrame(np.transpose(classifier.coef_), \n",
    "columns = [\"coef\"])],axis = 1)\n",
    "feature_importances.sort_values(\"coef\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 1:\n",
    " \n",
    "# Select Regularization Method   \n",
    "import time\n",
    "penalty = ['l1', 'l2']\n",
    "# Create regularization hyperparameter space\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "# Combine Parameters\n",
    "parameters = dict(C=C, penalty=penalty)\n",
    "lr_classifier = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = \"balanced_accuracy\",\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "t0 = time.time()\n",
    "lr_classifier  = lr_classifier .fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print(\"Took %0.2f seconds\" % (t1 - t0))\n",
    "lr_best_accuracy = lr_classifier.best_score_\n",
    "lr_best_parameters = lr_classifier.best_params_\n",
    "lr_best_accuracy, lr_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round 2:\n",
    "# Select Regularization Method\n",
    "import time\n",
    "penalty = ['l2']\n",
    "# Create regularization hyperparameter space\n",
    "C = [ 0.0001, 0.001, 0.01, 0.02, 0.05]\n",
    "# Combine Parameters\n",
    "parameters = dict(C=C, penalty=penalty)\n",
    "lr_classifier = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = \"balanced_accuracy\",\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "t0 = time.time()\n",
    "lr_classifier  = lr_classifier .fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "print(\"Took %0.2f seconds\" % (t1 - t0))\n",
    "lr_best_accuracy = lr_classifier.best_score_\n",
    "lr_best_parameters = lr_classifier.best_params_\n",
    "lr_best_accuracy, lr_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression(random_state = 0, penalty = 'l2')\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "# Predict the Test set results\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "#probability score\n",
    "y_pred_probs = lr_classifier.predict_proba(X_test)\n",
    "y_pred_probs  = y_pred_probs [:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revalidate final results with Confusion Matrix:\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "print (cm)\n",
    "#Confusion Matrix as a quick Crosstab:\n",
    "    \n",
    "pd.crosstab(y_test,pd.Series(y_pred),\n",
    "rownames=['ACTUAL'],colnames=['PRED'])\n",
    "#visualize Confusion Matrix:\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "df_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\n",
    "plt.figure(figsize = (28,20))\n",
    "fig, ax = plt.subplots()\n",
    "sn.set(font_scale=1.4)\n",
    "sn.heatmap(df_cm, annot=True, fmt='g'#,cmap=\"YlGnBu\" \n",
    "           )\n",
    "class_names=[0,1]\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix\\n', y=1.1)\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.ylabel('Actual label\\n')\n",
    "plt.xlabel('Predicted label\\n')\n",
    "print(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
